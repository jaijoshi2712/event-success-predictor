{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook contains all the exploration steps run on raw data before it is ready to ingest into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94169110-df75-47b7-9fc7-3ad0dd8238e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1518624-66c2-44d0-93f8-162014517fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 17:21:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/26 17:21:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/Users/priyangshupal/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.driver.memory','3g')\n",
    "conf.set('spark.ui.showConsoleProgress', False)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fcb5b-d24d-4943-b866-1a4ead4b1be6",
   "metadata": {},
   "source": [
    "### Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958ede6c-a779-4720-b7f3-bed859b08c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_data = spark.read.csv('../data/Twitter_Data.csv', multiLine=True, header=True).select(['clean_text', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717b21ab-6168-423b-8c56-e76d10fd06c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107760"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b621964-762e-4b5e-81d9-a5a07337d769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clean_text: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+\n",
      "|          clean_text|category|\n",
      "+--------------------+--------+\n",
      "|when modi promise...|    -1.0|\n",
      "|what did just say...|     1.0|\n",
      "|asking his suppor...|     1.0|\n",
      "|answer who among ...|     1.0|\n",
      "|with upcoming ele...|     1.0|\n",
      "|gandhi was gay do...|     1.0|\n",
      "|things like demon...|     1.0|\n",
      "|hope tuthukudi pe...|     1.0|\n",
      "|calm waters where...|     1.0|\n",
      "|vote such party a...|    -1.0|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_data.printSchema()\n",
    "twitter_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405ac7ec-1cd9-4b23-94cc-1a66289dd40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_data = twitter_data.withColumnRenamed('clean_text', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec6dbf-24e6-4c3f-b06b-fac13c9a1ba1",
   "metadata": {},
   "source": [
    "Checking for NULL rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.filter((F.col('category').isNull() | F.col('text').isNull())).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NULL rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4019ab5d-3cb2-4c6b-995b-1aac6411274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = twitter_data.filter(~(F.col('category').isNull() | F.col('text').isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f87f9-55fa-4c1a-b29a-e6d9346730d8",
   "metadata": {},
   "source": [
    "This DataFrame has 2 categories:\n",
    "\n",
    "-1 $\\Rightarrow$ Negative sentiment <br>\n",
    "1 $\\Rightarrow$ Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88799538-8b77-4f0c-8794-75a856e66acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|category|\n",
      "+--------+\n",
      "|     1.0|\n",
      "|    -1.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_data.select('category').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a6655-cdda-41b5-b0e7-be73e7b5b3b3",
   "metadata": {},
   "source": [
    "### Sentiment140 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c5952828-aff6-4558-a595-b54fede2fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140Schema = T.StructType([\n",
    "    T.StructField(\"target\", T.StringType(), True),        \n",
    "    T.StructField(\"id\", T.StringType(), True),\n",
    "    T.StructField(\"date\", T.StringType(), True),\n",
    "    T.StructField(\"flag\", T.StringType(), True),\n",
    "    T.StructField(\"user\", T.StringType(), True),\n",
    "    T.StructField(\"text\", T.StringType(), True),\n",
    "])\n",
    "sentiment140_spark = spark.read.csv('../data/sentiment140.csv', schema=sentiment140Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4fa2e30b-cb9b-4ce0-b259-b020452838bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['target', 'id', 'date', 'flag', 'user', 'text']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_spark.printSchema()\n",
    "sentiment140_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|        id|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|     0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|     0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|     0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140_spark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns ```id``` and ```user``` do not hold any such information that the mode can learn. So these columns can be dropped.\n",
    "\n",
    "We will check what other values the ```flag``` column holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    flag|\n",
      "+--------+\n",
      "|NO_QUERY|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140_spark.select('flag').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one unique entry in this column, our model cannot learn anything from this column. So we can drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_spark = sentiment140_spark.drop(*['id', 'user', 'flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'date', 'text']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_spark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will modify the ```date``` field to hold date time values that can be parsed easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mon Apr 06 22:19:45 PDT 2009'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_spark.select('date').first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating UDF to transform the ```date``` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "month_mapper = {month: index for index, month in enumerate(calendar.month_abbr) if month}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(date):\n",
    "  date_split = date.replace('PDT', '').split()[1:]\n",
    "  month = month_mapper[date_split[0]]\n",
    "  day = date_split[1]\n",
    "  time = date_split[2]\n",
    "  year = date_split[3]\n",
    "  date_string = str(month) + ' ' + str(day) + ' ' + str(time) + ' ' + str(year)\n",
    "  datetime_object = datetime.strptime(date_string, '%m %d %H:%M:%S %Y')\n",
    "  return str(datetime_object)  # printed in default format\n",
    "transform_date_udf = F.udf(transform_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_spark = sentiment140_spark\\\n",
    "  .withColumn('Date', transform_date_udf(F.col('date'))).select(['target', 'Date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------------------+\n",
      "|target|               Date|                text|\n",
      "+------+-------------------+--------------------+\n",
      "|     0|2009-04-06 22:19:45|@switchfoot http:...|\n",
      "|     0|2009-04-06 22:19:49|is upset that he ...|\n",
      "|     0|2009-04-06 22:19:53|@Kenichan I dived...|\n",
      "|     0|2009-04-06 22:19:57|my whole body fee...|\n",
      "|     0|2009-04-06 22:19:57|@nationwideclass ...|\n",
      "|     0|2009-04-06 22:20:00|@Kwesidei not the...|\n",
      "|     0|2009-04-06 22:20:03|         Need a hug |\n",
      "|     0|2009-04-06 22:20:03|@LOLTrish hey  lo...|\n",
      "|     0|2009-04-06 22:20:05|@Tatiana_K nope t...|\n",
      "|     0|2009-04-06 22:20:09|@twittera que me ...|\n",
      "+------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140_spark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframe has properly formatted date.\n",
    "\n",
    "We will now view how many unique values of ```target``` are there in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a995eed8-60cc-4bc0-8305-f59f5e94f0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|target| Count|\n",
      "+------+------+\n",
      "|     0|800000|\n",
      "|     4|800000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140_spark.groupBy('target').agg(F.count('target').alias('Count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has equal number of occurences of the 2 categories:\n",
    "\n",
    "0 $\\Rightarrow$ Negative sentiment <br>\n",
    "4 $\\Rightarrow$ Positive sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have common labels across dataframes $\\rightarrow 0$ will signify negative sentiment and $1$ will signify positive sentiment\n",
    "\n",
    "We will write a UDF to perform this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_label_transform(label_value, negative_sentiment, positive_sentiment):\n",
    "  return 0 if label_value == negative_sentiment else 1\n",
    "common_label_transform_udf = F.udf(common_label_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_data = twitter_data\\\n",
    "#   .withColumn('label', F.lit(common_label_transform_udf(F.col('category'), F.lit(\"-1.0\"), F.lit(\"1.0\"))))\\\n",
    "#   .select(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_spark = sentiment140_spark\\\n",
    "  .withColumn('label', F.lit(common_label_transform_udf(F.col('target'), F.lit(\"0\"), F.lit(\"4\"))))\\\n",
    "  .select(['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export all the preprocessed spark dataframes into database \n",
    "\n",
    "Storing in csv files for now $\\rightarrow$ (create the folder ```data-processed``` if it is not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_data.toPandas().to_csv('../data-processed/Twitter_Data.csv')\n",
    "sentiment140 = sentiment140_spark.toPandas().to_csv('../data-processed/sentiment140_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
