{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1771c9a8-20c6-4a19-84ca-5d7dbf2f67f5",
   "metadata": {},
   "source": [
    "#### This file contains the sentiment analysis model that will classify events as ```success``` or ```failure```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4017c48d-49d8-45a0-990c-833224419912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc756c81-6159-4e4b-ae3c-929b378df747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/03 21:44:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/priyangshupal/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.driver.memory','3g')\n",
    "conf.set('spark.ui.showConsoleProgress', False)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b471ad-0e6f-44a9-8e71-a94017fbe138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = spark.read.csv('../data-processed/sentiment140_data.csv', header=True).select(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9facc66-6177-4c49-931a-eaf6bc81f7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|@switchfoot http:...|    0|\n",
      "|is upset that he ...|    0|\n",
      "|@Kenichan I dived...|    0|\n",
      "|my whole body fee...|    0|\n",
      "|@nationwideclass ...|    0|\n",
      "|@Kwesidei not the...|    0|\n",
      "|         Need a hug |    0|\n",
      "|@LOLTrish hey  lo...|    0|\n",
      "|@Tatiana_K nope t...|    0|\n",
      "|@twittera que me ...|    0|\n",
      "|spring break in p...|    0|\n",
      "|I just re-pierced...|    0|\n",
      "|@caregiving I cou...|    0|\n",
      "|@octolinz16 It it...|    0|\n",
      "|@smarrison i woul...|    0|\n",
      "|@iamjazzyfizzle I...|    0|\n",
      "|Hollis' death sce...|    0|\n",
      "|about to file taxes |    0|\n",
      "|@LettyA ahh ive a...|    0|\n",
      "|@FakerPattyPattz ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fafa2-b1b8-41e7-acdd-0e19f269b263",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a30d7-a039-4a2a-b5ab-0aa275c4f36b",
   "metadata": {},
   "source": [
    "We will follow the following steps to prepare the data for our model\n",
    "1. Lowercase the text\n",
    "2. Remove stopwords from text\n",
    "3. Remove punctuations from text since that is noise and meaningful information cannot be learned from them\n",
    "4. Remove usernames, emojis, urls etc.\n",
    "5. Replace contractions\n",
    "6. Tokenize the text\n",
    "7. Perform stemming and lemmatization on text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1ce9b-1629-45e6-bb3a-27e089d18ed3",
   "metadata": {},
   "source": [
    "#### Lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709559a0-5aec-4235-a010-67784ea9f99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', F.lower(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9218b97-1998-4e25-8460-d83724b81194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|@switchfoot http:...|    0|\n",
      "|is upset that he ...|    0|\n",
      "|@kenichan i dived...|    0|\n",
      "|my whole body fee...|    0|\n",
      "|@nationwideclass ...|    0|\n",
      "|@kwesidei not the...|    0|\n",
      "|         need a hug |    0|\n",
      "|@loltrish hey  lo...|    0|\n",
      "|@tatiana_k nope t...|    0|\n",
      "|@twittera que me ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0915c9-c5d7-48a2-a3da-5ef32a06aa3f",
   "metadata": {},
   "source": [
    "#### Remove stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d945a9-e199-4730-85aa-7dc8e20ea65d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyangshupal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45383729-5b95-43a5-b352-5e669e523782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b072b17-de36-4045-a5f7-7de19d56581c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c36502-cc91-4f20-996c-6c9382c85af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "  return \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "\n",
    "removeStopwordsUDF = F.udf(removeStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e16a281-41fe-4038-9907-7957e13291d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', removeStopwordsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26473bd-bbe4-4708-a667-7efe27edb25f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|@switchfoot http:...|    0|\n",
      "|upset can't updat...|    0|\n",
      "|@kenichan dived m...|    0|\n",
      "|whole body feels ...|    0|\n",
      "|@nationwideclass ...|    0|\n",
      "|@kwesidei whole crew|    0|\n",
      "|            need hug|    0|\n",
      "|@loltrish hey lon...|    0|\n",
      "|     @tatiana_k nope|    0|\n",
      "|@twittera que mue...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8924a0-c499-4cb4-993f-4f6458be8e6c",
   "metadata": {},
   "source": [
    "#### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bc753e-2eae-436d-b848-c5ba0230c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4ec634-cbe0-4383-abec-7d9d4e188923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec52a0d-8bbf-4499-9417-05735170dcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    # Make a translation table that maps all punctuation characters to None\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the input string\n",
    "    return text.translate(translator)\n",
    "\n",
    "removePunctuationsUDF = F.udf(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e696b29-67c2-4ef8-95da-a60edc96ade6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', removePunctuationsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c6f3a3-9364-441d-bdf7-35d29d700430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|switchfoot httptw...|    0|\n",
      "|upset cant update...|    0|\n",
      "|kenichan dived ma...|    0|\n",
      "|whole body feels ...|    0|\n",
      "|nationwideclass n...|    0|\n",
      "| kwesidei whole crew|    0|\n",
      "|            need hug|    0|\n",
      "|loltrish hey long...|    0|\n",
      "|       tatianak nope|    0|\n",
      "| twittera que muera |    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda7b0c-5970-4047-8b38-b60ec8654293",
   "metadata": {},
   "source": [
    "#### Remove emails, emojis, urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47041df-af41-4b28-b046-962614db04f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55fef8ee-53d4-440b-bc3a-921f1a717a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_regex = '((www\\.[^\\s]+)|(https?://[^\\s]+))'\n",
    "username_regex = '@[^\\s]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6125183f-2ad8-49d1-82fc-55fbf81d661b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "  return re.sub(url_regex, '', text)\n",
    "\n",
    "def remove_usernames(text):\n",
    "  return re.sub(username_regex, '', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "  return emoji.demojize(text)\n",
    "\n",
    "remove_urlsUDF = F.udf(remove_urls)\n",
    "remove_usernamesUDF = F.udf(remove_usernames)\n",
    "remove_emojisUDF = F.udf(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "749cca71-0c7e-4d04-aaa0-a7b4a1880c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', remove_urlsUDF(F.col('text')))\n",
    "sentiment140 = sentiment140.withColumn('text', remove_usernamesUDF(F.col('text')))\n",
    "sentiment140 = sentiment140.withColumn('text', remove_emojisUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7e54cf7-7157-4a25-864f-23033552d26b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|switchfoot httptw...|    0|\n",
      "|upset cant update...|    0|\n",
      "|kenichan dived ma...|    0|\n",
      "|whole body feels ...|    0|\n",
      "|nationwideclass n...|    0|\n",
      "| kwesidei whole crew|    0|\n",
      "|            need hug|    0|\n",
      "|loltrish hey long...|    0|\n",
      "|       tatianak nope|    0|\n",
      "| twittera que muera |    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7431c-0dec-4b82-a23f-d57f3a55af8d",
   "metadata": {},
   "source": [
    "#### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f759df8-699d-47ea-81b0-16aab26f774c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e28caeb-c66a-4966-9cd3-bb3c343bd740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78bce0f1-68b1-4b66-b28a-85bebfe2d062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "tokenizeUDF = F.udf(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a5e4d2-9cf2-4c60-a7bb-fe91a0cfe99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', tokenizeUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42af3c5f-145b-4780-8784-1bdb04eeb65e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|[switchfoot, http...|    0|\n",
      "|[upset, cant, upd...|    0|\n",
      "|[kenichan, dived,...|    0|\n",
      "|[whole, body, fee...|    0|\n",
      "|[nationwideclass,...|    0|\n",
      "|[kwesidei, whole,...|    0|\n",
      "|         [need, hug]|    0|\n",
      "|[loltrish, hey, l...|    0|\n",
      "|    [tatianak, nope]|    0|\n",
      "|[twittera, que, m...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abff43-33ad-403d-b80c-af8778bc4d74",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933d5d9b-444b-46e7-87a2-33ffe6e32365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c6d254b-439e-416a-b237-e645e4fd41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc5009ae-5960-4e8e-9350-c66178bb8265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdbb0168-3ba7-42d9-8805-91af7a4c86a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "  return [stemmer.stem(word) for word in text]\n",
    "\n",
    "stemmingUDF = F.udf(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cec31d13-a769-42ed-8cfa-6f9d52cfc8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', stemmingUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff0fde95-e1f1-48cb-b053-1e6202801610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|[switchfoot, http...|    0|\n",
      "|[upset, cant, upd...|    0|\n",
      "|[kenichan, dive, ...|    0|\n",
      "|[whole, bodi, fee...|    0|\n",
      "|[nationwideclass,...|    0|\n",
      "|[kwesidei, whole,...|    0|\n",
      "|         [need, hug]|    0|\n",
      "|[loltrish, hey, l...|    0|\n",
      "|    [tatianak, nope]|    0|\n",
      "|[twittera, que, m...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e05ed7a-2d0c-4919-995c-1b3e8bdb613b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "317f8094-e18b-45c8-a306-08fcb2a55be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyangshupal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/priyangshupal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba205ffd-41e1-43e2-9005-30428320dac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "lemmatizeUDF = F.udf(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80142195-6a59-44be-9caa-a44bd0bea8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.withColumn('text', lemmatizeUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ce2e339-9d15-4af7-8c79-df8aadfef1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|[switchfoot, http...|    0|\n",
      "|[upset, cant, upd...|    0|\n",
      "|[kenichan, dive, ...|    0|\n",
      "|[whole, bodi, fee...|    0|\n",
      "|[nationwideclass,...|    0|\n",
      "|[kwesidei, whole,...|    0|\n",
      "|         [need, hug]|    0|\n",
      "|[loltrish, hey, l...|    0|\n",
      "|    [tatianak, nope]|    0|\n",
      "|[twittera, que, m...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment140.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140 = sentiment140.filter(~F.col('label').isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03a8ed-f56c-40f8-9b53-5ca5f75f6cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140.write.parquet(\"../data-processed/sentiment140_model_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
