{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0beb17b-aaae-4741-b7f5-7f37574858d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d00f6fd-04f4-47e2-a335-c6c150b1955a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",\n",
    "    client_secret=\"\",\n",
    "    password=\"\",\n",
    "    user_agent=\"\",\n",
    "    username=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfdd7be-3d21-48a1-9f89-d94324c43391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('RedditData').getOrCreate()\n",
    "\n",
    "subreddit_name = 'nyc'  # Change this to your target subreddit\n",
    "search_query = 'event'  # Modify this based on how events are typically posted\n",
    "six_months_ago = datetime.utcnow() - timedelta(days=6*30)  # Approximation of 6 months\n",
    "\n",
    "# Lists to store event and comments data\n",
    "events = []\n",
    "comments_data = []\n",
    "\n",
    "# Search the subreddit for posts containing 'event' in the title\n",
    "for submission in reddit.subreddit(subreddit_name).search(search_query, limit=10):  # Adjust the limit as needed\n",
    "    # if datetime.utcfromtimestamp(submission.created_utc) > six_months_ago:\n",
    "    submission.comments.replace_more(limit=None)  # Load all comments\n",
    "    event_info = (\n",
    "        submission.title,\n",
    "        submission.url,\n",
    "        datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        len(submission.comments)\n",
    "    )\n",
    "    events.append(event_info)\n",
    "\n",
    "    # Extract comments and add to comments_data\n",
    "    for comment in submission.comments.list():\n",
    "        comment_info = (submission.title, submission.url, comment.body)\n",
    "        comments_data.append(comment_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5971313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "events_df = pd.DataFrame(events, columns=['title', 'url', 'created', 'num_comments'])\n",
    "comments_df = pd.DataFrame(comments_data, columns=['title', 'url', 'comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f579160d-d0c2-4318-a974-43c7d44a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Spark DataFrames\n",
    "spark_events_df = spark.createDataFrame(events_df)\n",
    "spark_comments_df = spark.createDataFrame(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4607935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+------------+\n",
      "|               title|                 url|            created|num_comments|\n",
      "+--------------------+--------------------+-------------------+------------+\n",
      "|Rockefeller Cente...|https://www.reddi...|2023-11-30 05:50:02|          58|\n",
      "|Drama at a drag q...|https://v.redd.it...|2022-12-12 07:28:34|          73|\n",
      "|NYC Drag Story Ho...|https://www.nbcne...|2022-12-19 14:52:13|          47|\n",
      "|Eric Adams attend...|https://www.polit...|2023-11-17 13:41:16|          27|\n",
      "|NYC Mayor Adams i...|https://www.nydai...|2023-05-12 18:19:26|          28|\n",
      "|NYC's Newest Park...|https://i.redd.it...|2021-05-21 17:03:37|          40|\n",
      "|Republican Jewish...|https://www.haare...|2022-12-27 17:07:57|          35|\n",
      "|Trump Attends UFC...|https://www.theda...|2019-11-03 12:41:48|          30|\n",
      "|Ahead of potentia...|https://www.polit...|2023-02-19 17:28:21|          29|\n",
      "|'Change in percep...|https://www.nbcne...|2021-12-18 16:10:25|          30|\n",
      "+--------------------+--------------------+-------------------+------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               title|                 url|             comment|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Rockefeller Cente...|https://www.reddi...|I swear I feel li...|\n",
      "|Rockefeller Cente...|https://www.reddi...|What was the deal...|\n",
      "|Rockefeller Cente...|https://www.reddi...|The protest was o...|\n",
      "|Rockefeller Cente...|https://www.reddi...|Palestinians prot...|\n",
      "|Rockefeller Cente...|https://www.reddi...|God forbid people...|\n",
      "|Rockefeller Cente...|https://www.reddi...|What are they eve...|\n",
      "|Rockefeller Cente...|https://www.reddi...|God forbid we can...|\n",
      "|Rockefeller Cente...|https://www.reddi...|Jesus Christ I fe...|\n",
      "|Rockefeller Cente...|https://www.reddi...|As a Jew, we are ...|\n",
      "|Rockefeller Cente...|https://www.reddi...|I was there, and ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrames\n",
    "spark_events_df.show(10)\n",
    "spark_comments_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "987efd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = spark_comments_df.select(['comment']).withColumnRenamed('comment', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23ca5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', F.lower(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb022e",
   "metadata": {},
   "source": [
    "#### Remove stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fb3b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyangshupal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7af0e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab65a9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55a1a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "  return \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "\n",
    "removeStopwordsUDF = F.udf(removeStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fadd9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', removeStopwordsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2f1e5",
   "metadata": {},
   "source": [
    "#### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca19c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b27359a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a94b06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    # Make a translation table that maps all punctuation characters to None\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the input string\n",
    "    return text.translate(translator)\n",
    "\n",
    "removePunctuationsUDF = F.udf(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04afd9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', removePunctuationsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165be47",
   "metadata": {},
   "source": [
    "#### Remove emails, emojis, urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e052281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a1af136",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = '((www\\.[^\\s]+)|(https?://[^\\s]+))'\n",
    "username_regex = '@[^\\s]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "baa5fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "  return re.sub(url_regex, '', text)\n",
    "\n",
    "def remove_usernames(text):\n",
    "  return re.sub(username_regex, '', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "  return emoji.demojize(text)\n",
    "\n",
    "remove_urlsUDF = F.udf(remove_urls)\n",
    "remove_usernamesUDF = F.udf(remove_usernames)\n",
    "remove_emojisUDF = F.udf(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9480ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', remove_urlsUDF(F.col('text')))\n",
    "comments = comments.withColumn('text', remove_usernamesUDF(F.col('text')))\n",
    "comments = comments.withColumn('text', remove_emojisUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4b066",
   "metadata": {},
   "source": [
    "#### Tokenizing, stemming, and lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1c4369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99b9d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stem_lemmatize(text):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Stemming logic\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "    \n",
    "    # Lemmatizing logic\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos = 'a') for word in stemmed_words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "tokenize_stem_lemmatizeUDF = F.udf(tokenize_stem_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d7e99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', tokenize_stem_lemmatizeUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85f9176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|swear feel like e...|\n",
      "|deal propalestini...|\n",
      "|protest organ wit...|\n",
      "|palestinian prote...|\n",
      "|god forbid peopl ...|\n",
      "|even protest righ...|\n",
      "|god forbid enjoy ...|\n",
      "|jesu christ feel ...|\n",
      "|jew sorri guy did...|\n",
      "|   there disgust see|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comments.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799915c",
   "metadata": {},
   "source": [
    "#### Preparing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c397ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', F.split(F.col('text'), ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5ea2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='text', outputCol='embeddings', vocabSize=3216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84b50bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = cv.fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56ad19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = model.transform(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75e73d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|          embeddings|\n",
      "+--------------------+--------------------+\n",
      "|[swear, feel, lik...|(3216,[2,103,112,...|\n",
      "|[deal, propalesti...|(3216,[33,87,101,...|\n",
      "|[protest, organ, ...|(3216,[0,13,21,48...|\n",
      "|[palestinian, pro...|(3216,[8,13,111,1...|\n",
      "|[god, forbid, peo...|(3216,[0,6,41,62,...|\n",
      "|[even, protest, r...|(3216,[1,6,13,17,...|\n",
      "|[god, forbid, enj...|(3216,[12,90,216,...|\n",
      "|[jesu, christ, fe...|(3216,[4,8,13,15,...|\n",
      "|[jew, sorri, guy,...|(3216,[4,15,37,96...|\n",
      "|[there, disgust, ...|(3216,[19,39,1313...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16412d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
