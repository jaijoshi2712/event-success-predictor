{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file contains the sentiment analysis model that will classify events as ```success``` or ```failure```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af8f052-6459-414e-8cce-c0162bcb31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe895a-3528-4cc6-835e-757e81e96067",
   "metadata": {},
   "source": [
    "## Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fba9da0f-7aad-4aa7-a6c4-8136954b58eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "077379ae-6938-4117-a441-c0d51fd702c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140_sklearn = pd.read_parquet('../data-processed/sentiment140_model_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9423099-7021-45df-8ad2-2aafdb1efec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love health4uandpet u guy r best</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im meet one besti tonight cant wait girl talk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>darealsunisakim thank twitter add sunisa got m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sick realli cheap hurt much eat real food plu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lovesbrooklyn2 effect everyon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0                   love health4uandpet u guy r best     1\n",
       "1      im meet one besti tonight cant wait girl talk     1\n",
       "2  darealsunisakim thank twitter add sunisa got m...     1\n",
       "3  sick realli cheap hurt much eat real food plu ...     1\n",
       "4                      lovesbrooklyn2 effect everyon     1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_sklearn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49795dbf-e836-48d8-a2f1-54db0a3dbc06",
   "metadata": {},
   "source": [
    "#### Preparing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7570f3c-0261-4820-aae2-5d326dac5b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentiment140_sklearn['text'])\n",
    "y = sentiment140_sklearn['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "082728b4-0ff0-46f3-85e4-566065c0ccbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xtr, Xts, ytr, yts = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e72e9846-69d7-4a4e-af4a-000b1c443506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time_sklearn = time.time()\n",
    "model_sklearn = LogisticRegression(random_state=42, max_iter=100).fit(Xtr.astype(int), ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model_sklearn.predict(Xts.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sklearn = time.time() - start_time_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c651d8b-246d-4b95-8270-abf85002821f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse_sklearn = mean_squared_error(yts, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb25faa-ab76-4e9e-9ee4-1f3b1f746858",
   "metadata": {},
   "source": [
    "## Pyspark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3f0a27-c292-4aab-b873-7214d350649b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd4aa52-6969-4cae-983b-16dd46ed94dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/21 18:21:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/priyangshupal/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.driver.memory','8g')\n",
    "conf.set('spark.ui.showConsoleProgress', False)\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "    spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "except:\n",
    "    print('Spark context already exists, continuing with', sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5064d98f-e727-4a5a-94ad-4e3589a47d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment140_pyspark = spark.read.parquet('../data-processed/sentiment140_model_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b686e2-a7b4-42b2-a313-03ca002cd4f2",
   "metadata": {},
   "source": [
    "#### Preparing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dedebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|love health4uandp...|    1|[love, health4uan...|\n",
      "|im meet one besti...|    1|[im, meet, one, b...|\n",
      "|darealsunisakim t...|    1|[darealsunisakim,...|\n",
      "|sick realli cheap...|    1|[sick, realli, ch...|\n",
      "|lovesbrooklyn2 ef...|    1|[lovesbrooklyn2, ...|\n",
      "|productoffear tel...|    1|[productoffear, t...|\n",
      "|rkeithhil than re...|    1|[rkeithhil, than,...|\n",
      "|keepinupwkri jeal...|    1|[keepinupwkri, je...|\n",
      "|tommcfli ah congr...|    1|[tommcfli, ah, co...|\n",
      "|e4voip respond st...|    1|[e4voip, respond,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "sentiment140_pyspark = tokenizer.transform(sentiment140_pyspark)\n",
    "sentiment140_pyspark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e6505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|          embeddings|\n",
      "+--------------------+-----+--------------------+\n",
      "|love health4uandp...|    1|(262144,[51783,14...|\n",
      "|im meet one besti...|    1|(262144,[13781,21...|\n",
      "|darealsunisakim t...|    1|(262144,[1512,292...|\n",
      "|sick realli cheap...|    1|(262144,[60200,74...|\n",
      "|lovesbrooklyn2 ef...|    1|(262144,[31924,11...|\n",
      "|productoffear tel...|    1|(262144,[8940,124...|\n",
      "|rkeithhil than re...|    1|(262144,[892,1152...|\n",
      "|keepinupwkri jeal...|    1|(262144,[83747,12...|\n",
      "|tommcfli ah congr...|    1|(262144,[1512,522...|\n",
      "|e4voip respond st...|    1|(262144,[1696,581...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:21:30 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures')\n",
    "featurizedData = hashingTF.transform(sentiment140_pyspark)\n",
    "\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='embeddings')\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select('text', 'label', 'embeddings').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f1c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- embeddings: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData = rescaledData.withColumn('label', F.col('label').cast(T.IntegerType()))\n",
    "rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da86dc0f-bc00-4309-beda-41a01e93775a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentiment140_pyspark = sentiment140_pyspark.withColumn('label', F.col('label').cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09eba89d-29be-44dd-85fc-0a730c25cd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentiment140_pyspark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da2331ca-6a81-41bc-845f-f1e85243fbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = rescaledData.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2deb1059-90d9-4b5f-8b3f-779af5d0e5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_pyspark = LogisticRegression(maxIter=100, featuresCol='embeddings', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29ef2205-685d-4736-b8ed-650187ed2c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/12/21 18:21:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "start_time_pyspark = time.time()\n",
    "model_fitted_pyspark = model_pyspark.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a94aaa46-7dc9-4c4a-852b-5c1e8649ec37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predicted_pyspark = model_fitted_pyspark.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pyspark = time.time() - start_time_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14fe4989-02f4-4a85-9a62-a6e306662030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|21 day till chri ...|    1|         1|\n",
      "|a5hleyf im spend ...|    0|         0|\n",
      "|aaronrva bathroom...|    0|         0|\n",
      "|across univers sl...|    1|         0|\n",
      "|adriman roflmao n...|    1|         0|\n",
      "|             aghsnow|    0|         0|\n",
      "|ahh tedium fix br...|    1|         0|\n",
      "|albinla think ton...|    1|         1|\n",
      "|alicayaba cuuut h...|    0|         1|\n",
      "|allanatrogu thank...|    1|         1|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:21:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "model_predicted_pyspark.withColumn('prediction', F.col('prediction').cast(T.IntegerType()))\\\n",
    ".select(['text', 'label', 'prediction'])\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ead663c-4d0f-469a-a23b-06f08f8441bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:21:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='mse')\n",
    "mse_pyspark = evaluator.evaluate(model_predicted_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e328d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41406250000000006"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type      Training and inference time    Mean Squared Error\n",
      "------------  -----------------------------  --------------------\n",
      "Sklearn                            0.732677              0.5\n",
      "PySpark                            4.22457               0.414063\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  tabulate(\n",
    "    [['Sklearn', time_sklearn, mse_sklearn],\n",
    "    ['PySpark', time_pyspark, mse_pyspark]],\n",
    "    headers=['Model type', 'Training and inference time', 'Mean Squared Error']\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742456cf",
   "metadata": {},
   "source": [
    "## Scrape Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad639575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5a0c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",\n",
    "    client_secret=\"\",\n",
    "    password=\"\",\n",
    "    user_agent=\"\",\n",
    "    username=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e521d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName('RedditData').getOrCreate()\n",
    "\n",
    "subreddit_name = 'nyc'  # Change this to your target subreddit\n",
    "search_query = 'event'  # Modify this based on how events are typically posted\n",
    "six_months_ago = datetime.utcnow() - timedelta(days=6*30)  # Approximation of 6 months\n",
    "\n",
    "# Lists to store event and comments data\n",
    "events = []\n",
    "comments_data = []\n",
    "\n",
    "# Search the subreddit for posts containing 'event' in the title\n",
    "for submission in reddit.subreddit(subreddit_name).search(search_query, limit=10):  # Adjust the limit as needed\n",
    "    # if datetime.utcfromtimestamp(submission.created_utc) > six_months_ago:\n",
    "    submission.comments.replace_more(limit=None)  # Load all comments\n",
    "    event_info = (\n",
    "        submission.title,\n",
    "        submission.url,\n",
    "        datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        len(submission.comments)\n",
    "    )\n",
    "    events.append(event_info)\n",
    "\n",
    "    # Extract comments and add to comments_data\n",
    "    for comment in submission.comments.list():\n",
    "        comment_info = (submission.title, submission.url, comment.body)\n",
    "        comments_data.append(comment_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48eb4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "events_df = pd.DataFrame(events, columns=['title', 'url', 'created', 'num_comments'])\n",
    "comments_df = pd.DataFrame(comments_data, columns=['title', 'url', 'comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2643b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Spark DataFrames\n",
    "spark_events_df = spark.createDataFrame(events_df)\n",
    "spark_comments_df = spark.createDataFrame(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dd7b3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+------------+\n",
      "|               title|                 url|            created|num_comments|\n",
      "+--------------------+--------------------+-------------------+------------+\n",
      "|Rockefeller Cente...|https://www.reddi...|2023-11-30 05:50:02|          58|\n",
      "|Drama at a drag q...|https://v.redd.it...|2022-12-12 07:28:34|          73|\n",
      "|NYC Drag Story Ho...|https://www.nbcne...|2022-12-19 14:52:13|          47|\n",
      "|Eric Adams attend...|https://www.polit...|2023-11-17 13:41:16|          27|\n",
      "|NYC Mayor Adams i...|https://www.nydai...|2023-05-12 18:19:26|          28|\n",
      "|NYC's Newest Park...|https://i.redd.it...|2021-05-21 17:03:37|          40|\n",
      "|Republican Jewish...|https://www.haare...|2022-12-27 17:07:57|          35|\n",
      "|Trump Attends UFC...|https://www.theda...|2019-11-03 12:41:48|          30|\n",
      "|Ahead of potentia...|https://www.polit...|2023-02-19 17:28:21|          29|\n",
      "|'Change in percep...|https://www.nbcne...|2021-12-18 16:10:25|          30|\n",
      "+--------------------+--------------------+-------------------+------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               title|                 url|             comment|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Rockefeller Cente...|https://www.reddi...|I swear I feel li...|\n",
      "|Rockefeller Cente...|https://www.reddi...|What was the deal...|\n",
      "|Rockefeller Cente...|https://www.reddi...|The protest was o...|\n",
      "|Rockefeller Cente...|https://www.reddi...|Palestinians prot...|\n",
      "|Rockefeller Cente...|https://www.reddi...|God forbid people...|\n",
      "|Rockefeller Cente...|https://www.reddi...|What are they eve...|\n",
      "|Rockefeller Cente...|https://www.reddi...|God forbid we can...|\n",
      "|Rockefeller Cente...|https://www.reddi...|Jesus Christ I fe...|\n",
      "|Rockefeller Cente...|https://www.reddi...|As a Jew, we are ...|\n",
      "|Rockefeller Cente...|https://www.reddi...|I was there, and ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrames\n",
    "spark_events_df.show(10)\n",
    "spark_comments_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b5d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = spark_comments_df.select(['comment']).withColumnRenamed('comment', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19edb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', F.lower(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926728e3",
   "metadata": {},
   "source": [
    "#### Remove stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9689ffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyangshupal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ada0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8119a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2737d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "  return \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "\n",
    "removeStopwordsUDF = F.udf(removeStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0ce909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', removeStopwordsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a89e1",
   "metadata": {},
   "source": [
    "#### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "926f63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baf29b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf2c166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    # Make a translation table that maps all punctuation characters to None\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the input string\n",
    "    return text.translate(translator)\n",
    "\n",
    "removePunctuationsUDF = F.udf(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2b23786",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', removePunctuationsUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf933773",
   "metadata": {},
   "source": [
    "#### Remove emails, emojis, urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef44d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ec25d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = '((www\\.[^\\s]+)|(https?://[^\\s]+))'\n",
    "username_regex = '@[^\\s]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91e8d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "  return re.sub(url_regex, '', text)\n",
    "\n",
    "def remove_usernames(text):\n",
    "  return re.sub(username_regex, '', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "  return emoji.demojize(text)\n",
    "\n",
    "remove_urlsUDF = F.udf(remove_urls)\n",
    "remove_usernamesUDF = F.udf(remove_usernames)\n",
    "remove_emojisUDF = F.udf(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fcc9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', remove_urlsUDF(F.col('text')))\n",
    "comments = comments.withColumn('text', remove_usernamesUDF(F.col('text')))\n",
    "comments = comments.withColumn('text', remove_emojisUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487d1df",
   "metadata": {},
   "source": [
    "#### Tokenizing, stemming, and lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c00d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d213e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stem_lemmatize(text):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Stemming logic\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "    \n",
    "    # Lemmatizing logic\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos = 'a') for word in stemmed_words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "tokenize_stem_lemmatizeUDF = F.udf(tokenize_stem_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8b4d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', tokenize_stem_lemmatizeUDF(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cb51b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|swear feel like e...|\n",
      "|deal propalestini...|\n",
      "|protest organ wit...|\n",
      "|palestinian prote...|\n",
      "|god forbid peopl ...|\n",
      "|even protest righ...|\n",
      "|god forbid enjoy ...|\n",
      "|jesu christ feel ...|\n",
      "|jew sorri guy did...|\n",
      "|   there disgust see|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877ccb4",
   "metadata": {},
   "source": [
    "#### Preparing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0afd419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.withColumn('text', F.split(F.col('text'), ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0d1eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='text', outputCol='embeddings', vocabSize=3216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3e75f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a89404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = model.transform(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81ac26ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|          embeddings|\n",
      "+--------------------+--------------------+\n",
      "|[swear, feel, lik...|(3216,[2,104,113,...|\n",
      "|[deal, propalesti...|(3216,[33,87,102,...|\n",
      "|[protest, organ, ...|(3216,[0,13,21,48...|\n",
      "|[palestinian, pro...|(3216,[8,13,112,1...|\n",
      "|[god, forbid, peo...|(3216,[0,6,41,62,...|\n",
      "|[even, protest, r...|(3216,[1,6,13,17,...|\n",
      "|[god, forbid, enj...|(3216,[12,91,211,...|\n",
      "|[jesu, christ, fe...|(3216,[4,8,13,15,...|\n",
      "|[jew, sorri, guy,...|(3216,[4,15,37,96...|\n",
      "|[there, disgust, ...|(3216,[19,39,1407...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ce516d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_fitted_pyspark.transform(comments).groupBy('prediction')\\\n",
    ".count().orderBy(\"count\", ascending=False).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93d28ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure\n"
     ]
    }
   ],
   "source": [
    "print('Success' if prediction == 1.0 else 'Failure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
